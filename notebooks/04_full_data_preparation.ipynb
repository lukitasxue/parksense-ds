{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ParkSense: Full Dataset Preparation\n",
                "\n",
                "**Goal**: Process the **entire** 2019 dataset for maximum model accuracy.\n",
                "**Challenge**: The file is large (~3GB). We will use optimized types and chunking if necessary.\n",
                "\n",
                "**Steps**:\n",
                "1.  Load Static Data.\n",
                "2.  Load **ALL** Historical Data (2019).\n",
                "3.  Clean, Merge, and Feature Engineer.\n",
                "4.  Export to `processed_parking_data_full.csv`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.width', 1000)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Static Bays"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BAYS_PATH = '../data/on-street-parking-bays.csv'\n",
                "bays_df = pd.read_csv(BAYS_PATH)\n",
                "\n",
                "# Clean KerbsideID immediately\n",
                "bays_df = bays_df.dropna(subset=['KerbsideID'])\n",
                "bays_df['KerbsideID'] = bays_df['KerbsideID'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
                "\n",
                "print(f\"Bays loaded: {bays_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Full Historical Data (2019)\n",
                "We read the full CSV. To save memory, we can specify data types or read only needed columns initially, but let's try a standard load first. If it crashes, we will need to optimize."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SENSORS_PATH = '../data/On-street_Car_Parking_Sensor_Data_-_2019.csv'\n",
                "\n",
                "# Columns we definitely need (filtering early saves RAM)\n",
                "use_cols = ['ArrivalTime', 'DepartureTime', 'BayId', 'DurationMinutes']\n",
                "\n",
                "print(\"Loading FULL 2019 dataset... (this may take a minute)\")\n",
                "try:\n",
                "    sensors_df = pd.read_csv(\n",
                "        SENSORS_PATH,\n",
                "        usecols=use_cols,\n",
                "        parse_dates=['ArrivalTime', 'DepartureTime']\n",
                "    )\n",
                "    print(\"Load successful!\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading full dataset: {e}\")\n",
                "\n",
                "# Rename immediately\n",
                "sensors_df.rename(columns={\n",
                "    'ArrivalTime': 'Arrival_Time', \n",
                "    'DepartureTime': 'Departure_Time',\n",
                "    'BayId': 'KerbsideID',\n",
                "    'DurationMinutes': 'duration_min'\n",
                "}, inplace=True)\n",
                "\n",
                "print(f\"Full Sensors Shape: {sensors_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Cleaning & Merging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop NaNs\n",
                "sensors_df = sensors_df.dropna(subset=['KerbsideID', 'Arrival_Time', 'Departure_Time'])\n",
                "\n",
                "# Ensure String ID\n",
                "sensors_df['KerbsideID'] = sensors_df['KerbsideID'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
                "\n",
                "# Merge with Bays to get Location\n",
                "print(\"Merging datasets...\")\n",
                "merged_df = sensors_df.merge(\n",
                "    bays_df[['KerbsideID', 'Latitude', 'Longitude']], \n",
                "    on='KerbsideID', \n",
                "    how='inner'\n",
                ")\n",
                "\n",
                "print(f\"Merged Shape: {merged_df.shape}\")\n",
                "\n",
                "# Free up memory by deleting original dfs\n",
                "del sensors_df\n",
                "del bays_df\n",
                "import gc\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating features...\n",
                        "Features created.\n"
                    ]
                }
            ],
            "source": [
                "print(\"Generating features...\")\n",
                "merged_df['hour'] = merged_df['Arrival_Time'].dt.hour\n",
                "merged_df['day_of_week'] = merged_df['Arrival_Time'].dt.dayofweek\n",
                "merged_df['is_weekend'] = merged_df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
                "\n",
                "# Clean Duration: Convert to numeric, force errors to NaN\n",
                "merged_df['duration_min'] = pd.to_numeric(merged_df['duration_min'], errors='coerce')\n",
                "merged_df = merged_df.dropna(subset=['duration_min'])\n",
                "\n",
                "# Target Creation\n",
                "LOOKAHEAD_15 = 15\n",
                "merged_df['is_free_15m'] = (merged_df['duration_min'] < LOOKAHEAD_15).astype(int)\n",
                "\n",
                "print(\"Features created.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Export Full Dataset\n",
                "Since this file might be huge (millions of rows), we'll save it. Warning: The output CSV might also be large (>1GB)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saving to ../data/processed_parking_data_full.csv...\n",
                        "Done! Full dataset ready for training.\n"
                    ]
                }
            ],
            "source": [
                "OUTPUT_PATH = '../data/processed_parking_data_full.csv'\n",
                "print(f\"Saving to {OUTPUT_PATH}...\")\n",
                "merged_df.to_csv(OUTPUT_PATH, index=False)\n",
                "print(\"Done! Full dataset ready for training.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
